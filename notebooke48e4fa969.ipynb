{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-09T08:18:40.804951Z","iopub.execute_input":"2023-07-09T08:18:40.805629Z","iopub.status.idle":"2023-07-09T08:18:40.816993Z","shell.execute_reply.started":"2023-07-09T08:18:40.805573Z","shell.execute_reply":"2023-07-09T08:18:40.815608Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/input/body-performance-data/bodyPerformance.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/body-performance-data/bodyPerformance.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2023-07-09T08:18:40.819473Z","iopub.execute_input":"2023-07-09T08:18:40.820236Z","iopub.status.idle":"2023-07-09T08:18:40.894518Z","shell.execute_reply.started":"2023-07-09T08:18:40.820194Z","shell.execute_reply":"2023-07-09T08:18:40.893302Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"        age gender  height_cm  weight_kg  body fat_%  diastolic  systolic  \\\n0      27.0      M      172.3      75.24        21.3       80.0     130.0   \n1      25.0      M      165.0      55.80        15.7       77.0     126.0   \n2      31.0      M      179.6      78.00        20.1       92.0     152.0   \n3      32.0      M      174.5      71.10        18.4       76.0     147.0   \n4      28.0      M      173.8      67.70        17.1       70.0     127.0   \n...     ...    ...        ...        ...         ...        ...       ...   \n13388  25.0      M      172.1      71.80        16.2       74.0     141.0   \n13389  21.0      M      179.7      63.90        12.1       74.0     128.0   \n13390  39.0      M      177.2      80.50        20.1       78.0     132.0   \n13391  64.0      F      146.1      57.70        40.4       68.0     121.0   \n13392  34.0      M      164.0      66.10        19.5       82.0     150.0   \n\n       gripForce  sit and bend forward_cm  sit-ups counts  broad jump_cm class  \n0           54.9                     18.4            60.0          217.0     C  \n1           36.4                     16.3            53.0          229.0     A  \n2           44.8                     12.0            49.0          181.0     C  \n3           41.4                     15.2            53.0          219.0     B  \n4           43.5                     27.1            45.0          217.0     B  \n...          ...                      ...             ...            ...   ...  \n13388       35.8                     17.4            47.0          198.0     C  \n13389       33.0                      1.1            48.0          167.0     D  \n13390       63.5                     16.4            45.0          229.0     A  \n13391       19.3                      9.2             0.0           75.0     D  \n13392       35.9                      7.1            51.0          180.0     C  \n\n[13393 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>gender</th>\n      <th>height_cm</th>\n      <th>weight_kg</th>\n      <th>body fat_%</th>\n      <th>diastolic</th>\n      <th>systolic</th>\n      <th>gripForce</th>\n      <th>sit and bend forward_cm</th>\n      <th>sit-ups counts</th>\n      <th>broad jump_cm</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>27.0</td>\n      <td>M</td>\n      <td>172.3</td>\n      <td>75.24</td>\n      <td>21.3</td>\n      <td>80.0</td>\n      <td>130.0</td>\n      <td>54.9</td>\n      <td>18.4</td>\n      <td>60.0</td>\n      <td>217.0</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25.0</td>\n      <td>M</td>\n      <td>165.0</td>\n      <td>55.80</td>\n      <td>15.7</td>\n      <td>77.0</td>\n      <td>126.0</td>\n      <td>36.4</td>\n      <td>16.3</td>\n      <td>53.0</td>\n      <td>229.0</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>31.0</td>\n      <td>M</td>\n      <td>179.6</td>\n      <td>78.00</td>\n      <td>20.1</td>\n      <td>92.0</td>\n      <td>152.0</td>\n      <td>44.8</td>\n      <td>12.0</td>\n      <td>49.0</td>\n      <td>181.0</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>32.0</td>\n      <td>M</td>\n      <td>174.5</td>\n      <td>71.10</td>\n      <td>18.4</td>\n      <td>76.0</td>\n      <td>147.0</td>\n      <td>41.4</td>\n      <td>15.2</td>\n      <td>53.0</td>\n      <td>219.0</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28.0</td>\n      <td>M</td>\n      <td>173.8</td>\n      <td>67.70</td>\n      <td>17.1</td>\n      <td>70.0</td>\n      <td>127.0</td>\n      <td>43.5</td>\n      <td>27.1</td>\n      <td>45.0</td>\n      <td>217.0</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13388</th>\n      <td>25.0</td>\n      <td>M</td>\n      <td>172.1</td>\n      <td>71.80</td>\n      <td>16.2</td>\n      <td>74.0</td>\n      <td>141.0</td>\n      <td>35.8</td>\n      <td>17.4</td>\n      <td>47.0</td>\n      <td>198.0</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>13389</th>\n      <td>21.0</td>\n      <td>M</td>\n      <td>179.7</td>\n      <td>63.90</td>\n      <td>12.1</td>\n      <td>74.0</td>\n      <td>128.0</td>\n      <td>33.0</td>\n      <td>1.1</td>\n      <td>48.0</td>\n      <td>167.0</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>13390</th>\n      <td>39.0</td>\n      <td>M</td>\n      <td>177.2</td>\n      <td>80.50</td>\n      <td>20.1</td>\n      <td>78.0</td>\n      <td>132.0</td>\n      <td>63.5</td>\n      <td>16.4</td>\n      <td>45.0</td>\n      <td>229.0</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>13391</th>\n      <td>64.0</td>\n      <td>F</td>\n      <td>146.1</td>\n      <td>57.70</td>\n      <td>40.4</td>\n      <td>68.0</td>\n      <td>121.0</td>\n      <td>19.3</td>\n      <td>9.2</td>\n      <td>0.0</td>\n      <td>75.0</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>13392</th>\n      <td>34.0</td>\n      <td>M</td>\n      <td>164.0</td>\n      <td>66.10</td>\n      <td>19.5</td>\n      <td>82.0</td>\n      <td>150.0</td>\n      <td>35.9</td>\n      <td>7.1</td>\n      <td>51.0</td>\n      <td>180.0</td>\n      <td>C</td>\n    </tr>\n  </tbody>\n</table>\n<p>13393 rows × 12 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-07-09T08:18:40.897056Z","iopub.execute_input":"2023-07-09T08:18:40.897419Z","iopub.status.idle":"2023-07-09T08:18:40.908662Z","shell.execute_reply.started":"2023-07-09T08:18:40.897389Z","shell.execute_reply":"2023-07-09T08:18:40.907330Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"age                        float64\ngender                      object\nheight_cm                  float64\nweight_kg                  float64\nbody fat_%                 float64\ndiastolic                  float64\nsystolic                   float64\ngripForce                  float64\nsit and bend forward_cm    float64\nsit-ups counts             float64\nbroad jump_cm              float64\nclass                       object\ndtype: object"},"metadata":{}}]},{"cell_type":"code","source":"X = df.iloc[:, 0:11]\nX = pd.get_dummies(X).astype(float)\ny = df.iloc[:, 11]\n\nprint(X[0:5])\nprint(y[0:5])","metadata":{"execution":{"iopub.status.busy":"2023-07-09T08:20:20.484225Z","iopub.execute_input":"2023-07-09T08:20:20.484791Z","iopub.status.idle":"2023-07-09T08:20:20.514149Z","shell.execute_reply.started":"2023-07-09T08:20:20.484747Z","shell.execute_reply":"2023-07-09T08:20:20.512722Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"    age  height_cm  weight_kg  body fat_%  diastolic  systolic  gripForce  \\\n0  27.0      172.3      75.24        21.3       80.0     130.0       54.9   \n1  25.0      165.0      55.80        15.7       77.0     126.0       36.4   \n2  31.0      179.6      78.00        20.1       92.0     152.0       44.8   \n3  32.0      174.5      71.10        18.4       76.0     147.0       41.4   \n4  28.0      173.8      67.70        17.1       70.0     127.0       43.5   \n\n   sit and bend forward_cm  sit-ups counts  broad jump_cm  gender_F  gender_M  \n0                     18.4            60.0          217.0       0.0       1.0  \n1                     16.3            53.0          229.0       0.0       1.0  \n2                     12.0            49.0          181.0       0.0       1.0  \n3                     15.2            53.0          219.0       0.0       1.0  \n4                     27.1            45.0          217.0       0.0       1.0  \n0    C\n1    A\n2    C\n3    B\n4    B\nName: class, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"y = pd.get_dummies(y)\n\nprint(y[0:5])","metadata":{"execution":{"iopub.status.busy":"2023-07-09T08:20:24.937827Z","iopub.execute_input":"2023-07-09T08:20:24.938570Z","iopub.status.idle":"2023-07-09T08:20:24.949551Z","shell.execute_reply.started":"2023-07-09T08:20:24.938516Z","shell.execute_reply":"2023-07-09T08:20:24.948125Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"   A  B  C  D\n0  0  0  1  0\n1  1  0  0  0\n2  0  0  1  0\n3  0  1  0  0\n4  0  1  0  0\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# 학습과 테스트셋 구분\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n\n# 모델 설정\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=12, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(4, activation='softmax'))\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# 학습 중단 설정\nearly_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n\n# 저장 폴더와 이름\nmodelpath = \"./data/model/jh-bestmodel.hdf5\"\n\n#모델 업데이트하고 저장\ncheckpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n\n#모델 실행\nhistory = model.fit(X_train, y_train, epochs=1000, batch_size=10, validation_split=0.25, verbose=1, callbacks=[early_stopping_callback, checkpointer])\n\nscore = model.evaluate(X_test, y_test)\nprint('Test accuracy:', score[1])","metadata":{"execution":{"iopub.status.busy":"2023-07-09T08:33:34.536352Z","iopub.execute_input":"2023-07-09T08:33:34.536867Z","iopub.status.idle":"2023-07-09T08:42:19.976966Z","shell.execute_reply.started":"2023-07-09T08:33:34.536831Z","shell.execute_reply":"2023-07-09T08:42:19.975570Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Model: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_15 (Dense)            (None, 12)                156       \n                                                                 \n dense_16 (Dense)            (None, 8)                 104       \n                                                                 \n dense_17 (Dense)            (None, 4)                 36        \n                                                                 \n=================================================================\nTotal params: 296\nTrainable params: 296\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/1000\n804/804 [==============================] - 3s 3ms/step - loss: 2.0755 - accuracy: 0.2897 - val_loss: 1.3785 - val_accuracy: 0.2945\nEpoch 2/1000\n804/804 [==============================] - 2s 2ms/step - loss: 1.3668 - accuracy: 0.2951 - val_loss: 1.3566 - val_accuracy: 0.3139\nEpoch 3/1000\n804/804 [==============================] - 2s 2ms/step - loss: 1.3479 - accuracy: 0.3138 - val_loss: 1.3392 - val_accuracy: 0.3154\nEpoch 4/1000\n804/804 [==============================] - 2s 2ms/step - loss: 1.3053 - accuracy: 0.3713 - val_loss: 1.2299 - val_accuracy: 0.4487\nEpoch 5/1000\n804/804 [==============================] - 2s 2ms/step - loss: 1.1423 - accuracy: 0.4800 - val_loss: 1.0259 - val_accuracy: 0.5084\nEpoch 6/1000\n804/804 [==============================] - 2s 2ms/step - loss: 1.0107 - accuracy: 0.5238 - val_loss: 0.9526 - val_accuracy: 0.5405\nEpoch 7/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.9579 - accuracy: 0.5591 - val_loss: 0.9112 - val_accuracy: 0.5599\nEpoch 8/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.9400 - accuracy: 0.5652 - val_loss: 0.9178 - val_accuracy: 0.5838\nEpoch 9/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.9246 - accuracy: 0.5727 - val_loss: 0.8894 - val_accuracy: 0.5748\nEpoch 10/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.9213 - accuracy: 0.5773 - val_loss: 0.9512 - val_accuracy: 0.5692\nEpoch 11/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.9171 - accuracy: 0.5831 - val_loss: 0.8942 - val_accuracy: 0.5935\nEpoch 12/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.9133 - accuracy: 0.5808 - val_loss: 0.8722 - val_accuracy: 0.6028\nEpoch 13/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.9140 - accuracy: 0.5813 - val_loss: 0.8774 - val_accuracy: 0.5928\nEpoch 14/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.9099 - accuracy: 0.5847 - val_loss: 0.8737 - val_accuracy: 0.5898\nEpoch 15/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.9074 - accuracy: 0.5883 - val_loss: 0.8722 - val_accuracy: 0.5823\nEpoch 16/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.9042 - accuracy: 0.5915 - val_loss: 0.8899 - val_accuracy: 0.5849\nEpoch 17/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.9053 - accuracy: 0.5929 - val_loss: 0.8782 - val_accuracy: 0.6043\nEpoch 18/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.9029 - accuracy: 0.5914 - val_loss: 0.8958 - val_accuracy: 0.5898\nEpoch 19/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8997 - accuracy: 0.5925 - val_loss: 0.8664 - val_accuracy: 0.5965\nEpoch 20/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.9011 - accuracy: 0.5948 - val_loss: 0.8620 - val_accuracy: 0.6066\nEpoch 21/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8994 - accuracy: 0.5942 - val_loss: 0.8687 - val_accuracy: 0.6062\nEpoch 22/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.9004 - accuracy: 0.5925 - val_loss: 0.8680 - val_accuracy: 0.5991\nEpoch 23/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8973 - accuracy: 0.5970 - val_loss: 0.8692 - val_accuracy: 0.6043\nEpoch 24/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8955 - accuracy: 0.5943 - val_loss: 0.8598 - val_accuracy: 0.6032\nEpoch 25/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8959 - accuracy: 0.5909 - val_loss: 0.8659 - val_accuracy: 0.6032\nEpoch 26/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8913 - accuracy: 0.6025 - val_loss: 0.8650 - val_accuracy: 0.5946\nEpoch 27/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8946 - accuracy: 0.6026 - val_loss: 0.8610 - val_accuracy: 0.6054\nEpoch 28/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8918 - accuracy: 0.5963 - val_loss: 0.8723 - val_accuracy: 0.6051\nEpoch 29/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8899 - accuracy: 0.5991 - val_loss: 0.8816 - val_accuracy: 0.6032\nEpoch 30/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8903 - accuracy: 0.6015 - val_loss: 0.8620 - val_accuracy: 0.6017\nEpoch 31/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8924 - accuracy: 0.5994 - val_loss: 0.8604 - val_accuracy: 0.6137\nEpoch 32/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8889 - accuracy: 0.5961 - val_loss: 0.8912 - val_accuracy: 0.5950\nEpoch 33/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8903 - accuracy: 0.5964 - val_loss: 0.8632 - val_accuracy: 0.6021\nEpoch 34/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8939 - accuracy: 0.5954 - val_loss: 0.8658 - val_accuracy: 0.5943\nEpoch 35/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8905 - accuracy: 0.5981 - val_loss: 0.8778 - val_accuracy: 0.5935\nEpoch 36/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8909 - accuracy: 0.5991 - val_loss: 0.8639 - val_accuracy: 0.5894\nEpoch 37/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8896 - accuracy: 0.5943 - val_loss: 0.8637 - val_accuracy: 0.6096\nEpoch 38/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8897 - accuracy: 0.6014 - val_loss: 0.8871 - val_accuracy: 0.5961\nEpoch 39/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8889 - accuracy: 0.5958 - val_loss: 0.8743 - val_accuracy: 0.6002\nEpoch 40/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8894 - accuracy: 0.6000 - val_loss: 0.8603 - val_accuracy: 0.6069\nEpoch 41/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8884 - accuracy: 0.6016 - val_loss: 0.8544 - val_accuracy: 0.6081\nEpoch 42/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8881 - accuracy: 0.6001 - val_loss: 0.8625 - val_accuracy: 0.6069\nEpoch 43/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8873 - accuracy: 0.6006 - val_loss: 0.8699 - val_accuracy: 0.6066\nEpoch 44/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8876 - accuracy: 0.5961 - val_loss: 0.8561 - val_accuracy: 0.6084\nEpoch 45/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8898 - accuracy: 0.6027 - val_loss: 0.8576 - val_accuracy: 0.6069\nEpoch 46/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8862 - accuracy: 0.6017 - val_loss: 0.8600 - val_accuracy: 0.6040\nEpoch 47/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8867 - accuracy: 0.6037 - val_loss: 0.8582 - val_accuracy: 0.6066\nEpoch 48/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8911 - accuracy: 0.5951 - val_loss: 0.8673 - val_accuracy: 0.5946\nEpoch 49/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8885 - accuracy: 0.6070 - val_loss: 0.8580 - val_accuracy: 0.6099\nEpoch 50/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8874 - accuracy: 0.6009 - val_loss: 0.8643 - val_accuracy: 0.6006\nEpoch 51/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8861 - accuracy: 0.5948 - val_loss: 0.8597 - val_accuracy: 0.6036\nEpoch 52/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8868 - accuracy: 0.6021 - val_loss: 0.8574 - val_accuracy: 0.6077\nEpoch 53/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8856 - accuracy: 0.6051 - val_loss: 0.8540 - val_accuracy: 0.6047\nEpoch 54/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8848 - accuracy: 0.6000 - val_loss: 0.8570 - val_accuracy: 0.6099\nEpoch 55/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8871 - accuracy: 0.5969 - val_loss: 0.8519 - val_accuracy: 0.6096\nEpoch 56/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8852 - accuracy: 0.6052 - val_loss: 0.8508 - val_accuracy: 0.6110\nEpoch 57/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8809 - accuracy: 0.6044 - val_loss: 0.9047 - val_accuracy: 0.5860\nEpoch 58/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8850 - accuracy: 0.6045 - val_loss: 0.9022 - val_accuracy: 0.5943\nEpoch 59/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8865 - accuracy: 0.6019 - val_loss: 0.8510 - val_accuracy: 0.6069\nEpoch 60/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8808 - accuracy: 0.6098 - val_loss: 0.8495 - val_accuracy: 0.6099\nEpoch 61/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8847 - accuracy: 0.6019 - val_loss: 0.8621 - val_accuracy: 0.6054\nEpoch 62/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8833 - accuracy: 0.6030 - val_loss: 0.8953 - val_accuracy: 0.5935\nEpoch 63/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8819 - accuracy: 0.6004 - val_loss: 0.8563 - val_accuracy: 0.6043\nEpoch 64/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8792 - accuracy: 0.6056 - val_loss: 0.8561 - val_accuracy: 0.6110\nEpoch 65/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8832 - accuracy: 0.6022 - val_loss: 0.8480 - val_accuracy: 0.6148\nEpoch 66/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8773 - accuracy: 0.6097 - val_loss: 0.8491 - val_accuracy: 0.6170\nEpoch 67/1000\n804/804 [==============================] - 2s 3ms/step - loss: 0.8790 - accuracy: 0.6057 - val_loss: 0.8468 - val_accuracy: 0.6125\nEpoch 68/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8779 - accuracy: 0.6075 - val_loss: 0.8694 - val_accuracy: 0.6077\nEpoch 69/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8806 - accuracy: 0.6092 - val_loss: 0.8526 - val_accuracy: 0.6144\nEpoch 70/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8798 - accuracy: 0.6087 - val_loss: 0.8669 - val_accuracy: 0.6066\nEpoch 71/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8776 - accuracy: 0.6113 - val_loss: 0.8498 - val_accuracy: 0.6166\nEpoch 72/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8786 - accuracy: 0.6067 - val_loss: 0.9338 - val_accuracy: 0.5722\nEpoch 73/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8774 - accuracy: 0.6047 - val_loss: 0.8515 - val_accuracy: 0.6107\nEpoch 74/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8740 - accuracy: 0.6119 - val_loss: 0.8520 - val_accuracy: 0.6152\nEpoch 75/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8761 - accuracy: 0.6020 - val_loss: 0.8512 - val_accuracy: 0.6140\nEpoch 76/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8776 - accuracy: 0.6075 - val_loss: 0.8467 - val_accuracy: 0.6148\nEpoch 77/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8775 - accuracy: 0.6042 - val_loss: 0.8512 - val_accuracy: 0.6166\nEpoch 78/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8752 - accuracy: 0.6029 - val_loss: 0.8537 - val_accuracy: 0.6152\nEpoch 79/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8716 - accuracy: 0.6112 - val_loss: 0.8444 - val_accuracy: 0.6178\nEpoch 80/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8736 - accuracy: 0.6073 - val_loss: 0.8531 - val_accuracy: 0.6137\nEpoch 81/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8715 - accuracy: 0.6075 - val_loss: 0.8481 - val_accuracy: 0.6133\nEpoch 82/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8730 - accuracy: 0.6111 - val_loss: 0.8413 - val_accuracy: 0.6193\nEpoch 83/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8730 - accuracy: 0.6082 - val_loss: 0.8408 - val_accuracy: 0.6163\nEpoch 84/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8724 - accuracy: 0.6098 - val_loss: 0.8533 - val_accuracy: 0.6163\nEpoch 85/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8751 - accuracy: 0.6068 - val_loss: 0.8471 - val_accuracy: 0.6155\nEpoch 86/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8714 - accuracy: 0.6106 - val_loss: 0.8699 - val_accuracy: 0.6069\nEpoch 87/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8683 - accuracy: 0.6110 - val_loss: 0.8422 - val_accuracy: 0.6166\nEpoch 88/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8687 - accuracy: 0.6092 - val_loss: 0.8438 - val_accuracy: 0.6208\nEpoch 89/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8681 - accuracy: 0.6081 - val_loss: 0.8414 - val_accuracy: 0.6140\nEpoch 90/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8673 - accuracy: 0.6114 - val_loss: 0.8856 - val_accuracy: 0.6028\nEpoch 91/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8686 - accuracy: 0.6144 - val_loss: 0.8744 - val_accuracy: 0.6021\nEpoch 92/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8693 - accuracy: 0.6108 - val_loss: 0.8722 - val_accuracy: 0.5991\nEpoch 93/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8650 - accuracy: 0.6139 - val_loss: 0.8551 - val_accuracy: 0.6107\nEpoch 94/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8657 - accuracy: 0.6149 - val_loss: 0.8385 - val_accuracy: 0.6230\nEpoch 95/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8658 - accuracy: 0.6132 - val_loss: 0.8453 - val_accuracy: 0.6144\nEpoch 96/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8639 - accuracy: 0.6081 - val_loss: 0.8992 - val_accuracy: 0.5819\nEpoch 97/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8665 - accuracy: 0.6072 - val_loss: 0.8804 - val_accuracy: 0.5995\nEpoch 98/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8676 - accuracy: 0.6105 - val_loss: 0.8391 - val_accuracy: 0.6170\nEpoch 99/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8661 - accuracy: 0.6112 - val_loss: 0.8459 - val_accuracy: 0.6166\nEpoch 100/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8677 - accuracy: 0.6102 - val_loss: 0.8447 - val_accuracy: 0.6155\nEpoch 101/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8631 - accuracy: 0.6112 - val_loss: 0.8390 - val_accuracy: 0.6208\nEpoch 102/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8657 - accuracy: 0.6112 - val_loss: 0.8489 - val_accuracy: 0.6144\nEpoch 103/1000\n804/804 [==============================] - 2s 3ms/step - loss: 0.8635 - accuracy: 0.6142 - val_loss: 0.8383 - val_accuracy: 0.6208\nEpoch 104/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8634 - accuracy: 0.6121 - val_loss: 0.8396 - val_accuracy: 0.6200\nEpoch 105/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8651 - accuracy: 0.6136 - val_loss: 0.8498 - val_accuracy: 0.6178\nEpoch 106/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8625 - accuracy: 0.6156 - val_loss: 0.8387 - val_accuracy: 0.6196\nEpoch 107/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8623 - accuracy: 0.6103 - val_loss: 0.8392 - val_accuracy: 0.6174\nEpoch 108/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8662 - accuracy: 0.6123 - val_loss: 0.8499 - val_accuracy: 0.6092\nEpoch 109/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8645 - accuracy: 0.6098 - val_loss: 0.8421 - val_accuracy: 0.6208\nEpoch 110/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8617 - accuracy: 0.6127 - val_loss: 0.8357 - val_accuracy: 0.6174\nEpoch 111/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8624 - accuracy: 0.6101 - val_loss: 0.8580 - val_accuracy: 0.6069\nEpoch 112/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8599 - accuracy: 0.6095 - val_loss: 0.8547 - val_accuracy: 0.6099\nEpoch 113/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8644 - accuracy: 0.6108 - val_loss: 0.8486 - val_accuracy: 0.6122\nEpoch 114/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8618 - accuracy: 0.6139 - val_loss: 0.8374 - val_accuracy: 0.6189\nEpoch 115/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8619 - accuracy: 0.6132 - val_loss: 0.8480 - val_accuracy: 0.6144\nEpoch 116/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8609 - accuracy: 0.6085 - val_loss: 0.8498 - val_accuracy: 0.6174\nEpoch 117/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8633 - accuracy: 0.6102 - val_loss: 0.8362 - val_accuracy: 0.6215\nEpoch 118/1000\n804/804 [==============================] - 2s 3ms/step - loss: 0.8605 - accuracy: 0.6158 - val_loss: 0.8353 - val_accuracy: 0.6189\nEpoch 119/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8601 - accuracy: 0.6143 - val_loss: 0.8440 - val_accuracy: 0.6215\nEpoch 120/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8608 - accuracy: 0.6136 - val_loss: 0.8365 - val_accuracy: 0.6170\nEpoch 121/1000\n804/804 [==============================] - 2s 3ms/step - loss: 0.8580 - accuracy: 0.6161 - val_loss: 0.8347 - val_accuracy: 0.6152\nEpoch 122/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8632 - accuracy: 0.6184 - val_loss: 0.8382 - val_accuracy: 0.6200\nEpoch 123/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8592 - accuracy: 0.6173 - val_loss: 0.8356 - val_accuracy: 0.6196\nEpoch 124/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8591 - accuracy: 0.6174 - val_loss: 0.8549 - val_accuracy: 0.6185\nEpoch 125/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8570 - accuracy: 0.6156 - val_loss: 0.8323 - val_accuracy: 0.6204\nEpoch 126/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8581 - accuracy: 0.6180 - val_loss: 0.8365 - val_accuracy: 0.6226\nEpoch 127/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8576 - accuracy: 0.6139 - val_loss: 0.8361 - val_accuracy: 0.6193\nEpoch 128/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8602 - accuracy: 0.6129 - val_loss: 0.8384 - val_accuracy: 0.6204\nEpoch 129/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8604 - accuracy: 0.6151 - val_loss: 0.8327 - val_accuracy: 0.6193\nEpoch 130/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8594 - accuracy: 0.6139 - val_loss: 0.8410 - val_accuracy: 0.6241\nEpoch 131/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8591 - accuracy: 0.6172 - val_loss: 0.8317 - val_accuracy: 0.6245\nEpoch 132/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8569 - accuracy: 0.6136 - val_loss: 0.8354 - val_accuracy: 0.6230\nEpoch 133/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8547 - accuracy: 0.6169 - val_loss: 0.8324 - val_accuracy: 0.6267\nEpoch 134/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8579 - accuracy: 0.6144 - val_loss: 0.8535 - val_accuracy: 0.6222\nEpoch 135/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8558 - accuracy: 0.6153 - val_loss: 0.8406 - val_accuracy: 0.6181\nEpoch 136/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8550 - accuracy: 0.6162 - val_loss: 0.8324 - val_accuracy: 0.6204\nEpoch 137/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8543 - accuracy: 0.6172 - val_loss: 0.8577 - val_accuracy: 0.6148\nEpoch 138/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8558 - accuracy: 0.6128 - val_loss: 0.8468 - val_accuracy: 0.6103\nEpoch 139/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8548 - accuracy: 0.6154 - val_loss: 0.8388 - val_accuracy: 0.6140\nEpoch 140/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8547 - accuracy: 0.6161 - val_loss: 0.8291 - val_accuracy: 0.6237\nEpoch 141/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8526 - accuracy: 0.6179 - val_loss: 0.8360 - val_accuracy: 0.6267\nEpoch 142/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8552 - accuracy: 0.6149 - val_loss: 0.8289 - val_accuracy: 0.6290\nEpoch 143/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8566 - accuracy: 0.6124 - val_loss: 0.8308 - val_accuracy: 0.6219\nEpoch 144/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8522 - accuracy: 0.6188 - val_loss: 0.8320 - val_accuracy: 0.6219\nEpoch 145/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8537 - accuracy: 0.6202 - val_loss: 0.8546 - val_accuracy: 0.6166\nEpoch 146/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8562 - accuracy: 0.6136 - val_loss: 0.8339 - val_accuracy: 0.6193\nEpoch 147/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8536 - accuracy: 0.6156 - val_loss: 0.8376 - val_accuracy: 0.6144\nEpoch 148/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8521 - accuracy: 0.6182 - val_loss: 0.8312 - val_accuracy: 0.6267\nEpoch 149/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8526 - accuracy: 0.6157 - val_loss: 0.8285 - val_accuracy: 0.6181\nEpoch 150/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8534 - accuracy: 0.6161 - val_loss: 0.8260 - val_accuracy: 0.6230\nEpoch 151/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8530 - accuracy: 0.6159 - val_loss: 0.8321 - val_accuracy: 0.6245\nEpoch 152/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8524 - accuracy: 0.6178 - val_loss: 0.8297 - val_accuracy: 0.6282\nEpoch 153/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8492 - accuracy: 0.6218 - val_loss: 0.8340 - val_accuracy: 0.6181\nEpoch 154/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8564 - accuracy: 0.6175 - val_loss: 0.8342 - val_accuracy: 0.6290\nEpoch 155/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8512 - accuracy: 0.6182 - val_loss: 0.8343 - val_accuracy: 0.6301\nEpoch 156/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8508 - accuracy: 0.6158 - val_loss: 0.8238 - val_accuracy: 0.6211\nEpoch 157/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8528 - accuracy: 0.6137 - val_loss: 0.8430 - val_accuracy: 0.6215\nEpoch 158/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8491 - accuracy: 0.6184 - val_loss: 0.8295 - val_accuracy: 0.6208\nEpoch 159/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8510 - accuracy: 0.6179 - val_loss: 0.8437 - val_accuracy: 0.6230\nEpoch 160/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8485 - accuracy: 0.6174 - val_loss: 0.8243 - val_accuracy: 0.6241\nEpoch 161/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8508 - accuracy: 0.6217 - val_loss: 0.8261 - val_accuracy: 0.6260\nEpoch 162/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8505 - accuracy: 0.6189 - val_loss: 0.8358 - val_accuracy: 0.6245\nEpoch 163/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8493 - accuracy: 0.6178 - val_loss: 0.8331 - val_accuracy: 0.6200\nEpoch 164/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8473 - accuracy: 0.6152 - val_loss: 0.8223 - val_accuracy: 0.6208\nEpoch 165/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8493 - accuracy: 0.6199 - val_loss: 0.8237 - val_accuracy: 0.6241\nEpoch 166/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8475 - accuracy: 0.6225 - val_loss: 0.8394 - val_accuracy: 0.6208\nEpoch 167/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8469 - accuracy: 0.6143 - val_loss: 0.8253 - val_accuracy: 0.6256\nEpoch 168/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8500 - accuracy: 0.6218 - val_loss: 0.8293 - val_accuracy: 0.6181\nEpoch 169/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8496 - accuracy: 0.6161 - val_loss: 0.8481 - val_accuracy: 0.6096\nEpoch 170/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8473 - accuracy: 0.6208 - val_loss: 0.8278 - val_accuracy: 0.6297\nEpoch 171/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8467 - accuracy: 0.6254 - val_loss: 0.8251 - val_accuracy: 0.6286\nEpoch 172/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8470 - accuracy: 0.6194 - val_loss: 0.8226 - val_accuracy: 0.6278\nEpoch 173/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8464 - accuracy: 0.6169 - val_loss: 0.8313 - val_accuracy: 0.6241\nEpoch 174/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8454 - accuracy: 0.6192 - val_loss: 0.8286 - val_accuracy: 0.6349\nEpoch 175/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8482 - accuracy: 0.6208 - val_loss: 0.8228 - val_accuracy: 0.6215\nEpoch 176/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8468 - accuracy: 0.6231 - val_loss: 0.8185 - val_accuracy: 0.6278\nEpoch 177/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8441 - accuracy: 0.6182 - val_loss: 0.8175 - val_accuracy: 0.6260\nEpoch 178/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8473 - accuracy: 0.6177 - val_loss: 0.8194 - val_accuracy: 0.6219\nEpoch 179/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8431 - accuracy: 0.6202 - val_loss: 0.8152 - val_accuracy: 0.6338\nEpoch 180/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8454 - accuracy: 0.6189 - val_loss: 0.8316 - val_accuracy: 0.6196\nEpoch 181/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8451 - accuracy: 0.6184 - val_loss: 0.8171 - val_accuracy: 0.6327\nEpoch 182/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8477 - accuracy: 0.6198 - val_loss: 0.8291 - val_accuracy: 0.6178\nEpoch 183/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8438 - accuracy: 0.6149 - val_loss: 0.8196 - val_accuracy: 0.6286\nEpoch 184/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8399 - accuracy: 0.6215 - val_loss: 0.8192 - val_accuracy: 0.6193\nEpoch 185/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8429 - accuracy: 0.6204 - val_loss: 0.8094 - val_accuracy: 0.6323\nEpoch 186/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8413 - accuracy: 0.6236 - val_loss: 0.8129 - val_accuracy: 0.6346\nEpoch 187/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8462 - accuracy: 0.6199 - val_loss: 0.8282 - val_accuracy: 0.6286\nEpoch 188/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8399 - accuracy: 0.6205 - val_loss: 0.8088 - val_accuracy: 0.6338\nEpoch 189/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8398 - accuracy: 0.6156 - val_loss: 0.8129 - val_accuracy: 0.6342\nEpoch 190/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8384 - accuracy: 0.6228 - val_loss: 0.8199 - val_accuracy: 0.6226\nEpoch 191/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8389 - accuracy: 0.6217 - val_loss: 0.8222 - val_accuracy: 0.6308\nEpoch 192/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8387 - accuracy: 0.6185 - val_loss: 0.8052 - val_accuracy: 0.6405\nEpoch 193/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8370 - accuracy: 0.6273 - val_loss: 0.8102 - val_accuracy: 0.6334\nEpoch 194/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8370 - accuracy: 0.6189 - val_loss: 0.8168 - val_accuracy: 0.6245\nEpoch 195/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8383 - accuracy: 0.6235 - val_loss: 0.8076 - val_accuracy: 0.6372\nEpoch 196/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8355 - accuracy: 0.6213 - val_loss: 0.8151 - val_accuracy: 0.6320\nEpoch 197/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8366 - accuracy: 0.6220 - val_loss: 0.8045 - val_accuracy: 0.6349\nEpoch 198/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8369 - accuracy: 0.6227 - val_loss: 0.8059 - val_accuracy: 0.6405\nEpoch 199/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8339 - accuracy: 0.6236 - val_loss: 0.8110 - val_accuracy: 0.6308\nEpoch 200/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8369 - accuracy: 0.6231 - val_loss: 0.8055 - val_accuracy: 0.6439\nEpoch 201/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8332 - accuracy: 0.6217 - val_loss: 0.8038 - val_accuracy: 0.6383\nEpoch 202/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8331 - accuracy: 0.6254 - val_loss: 0.8166 - val_accuracy: 0.6200\nEpoch 203/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8338 - accuracy: 0.6192 - val_loss: 0.8084 - val_accuracy: 0.6293\nEpoch 204/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8332 - accuracy: 0.6243 - val_loss: 0.7992 - val_accuracy: 0.6424\nEpoch 205/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8302 - accuracy: 0.6192 - val_loss: 0.8046 - val_accuracy: 0.6353\nEpoch 206/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8316 - accuracy: 0.6273 - val_loss: 0.7961 - val_accuracy: 0.6424\nEpoch 207/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8323 - accuracy: 0.6259 - val_loss: 0.8004 - val_accuracy: 0.6379\nEpoch 208/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8308 - accuracy: 0.6250 - val_loss: 0.8033 - val_accuracy: 0.6383\nEpoch 209/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8307 - accuracy: 0.6253 - val_loss: 0.8218 - val_accuracy: 0.6226\nEpoch 210/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8315 - accuracy: 0.6230 - val_loss: 0.8115 - val_accuracy: 0.6349\nEpoch 211/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8312 - accuracy: 0.6250 - val_loss: 0.7993 - val_accuracy: 0.6409\nEpoch 212/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8326 - accuracy: 0.6241 - val_loss: 0.8063 - val_accuracy: 0.6260\nEpoch 213/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8298 - accuracy: 0.6256 - val_loss: 0.7982 - val_accuracy: 0.6428\nEpoch 214/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8319 - accuracy: 0.6296 - val_loss: 0.8697 - val_accuracy: 0.6047\nEpoch 215/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8282 - accuracy: 0.6286 - val_loss: 0.7988 - val_accuracy: 0.6417\nEpoch 216/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8270 - accuracy: 0.6261 - val_loss: 0.8082 - val_accuracy: 0.6323\nEpoch 217/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8248 - accuracy: 0.6289 - val_loss: 0.8009 - val_accuracy: 0.6465\nEpoch 218/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8238 - accuracy: 0.6279 - val_loss: 0.8010 - val_accuracy: 0.6331\nEpoch 219/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8257 - accuracy: 0.6269 - val_loss: 0.7965 - val_accuracy: 0.6376\nEpoch 220/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8252 - accuracy: 0.6296 - val_loss: 0.8076 - val_accuracy: 0.6398\nEpoch 221/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8258 - accuracy: 0.6286 - val_loss: 0.7966 - val_accuracy: 0.6376\nEpoch 222/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8210 - accuracy: 0.6317 - val_loss: 0.8095 - val_accuracy: 0.6342\nEpoch 223/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8226 - accuracy: 0.6319 - val_loss: 0.7959 - val_accuracy: 0.6379\nEpoch 224/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8193 - accuracy: 0.6317 - val_loss: 0.8276 - val_accuracy: 0.6252\nEpoch 225/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8291 - accuracy: 0.6284 - val_loss: 0.8049 - val_accuracy: 0.6379\nEpoch 226/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8225 - accuracy: 0.6314 - val_loss: 0.8058 - val_accuracy: 0.6420\nEpoch 227/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8222 - accuracy: 0.6302 - val_loss: 0.7990 - val_accuracy: 0.6405\nEpoch 228/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8199 - accuracy: 0.6365 - val_loss: 0.7916 - val_accuracy: 0.6446\nEpoch 229/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8212 - accuracy: 0.6309 - val_loss: 0.7916 - val_accuracy: 0.6465\nEpoch 230/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8162 - accuracy: 0.6324 - val_loss: 0.8082 - val_accuracy: 0.6364\nEpoch 231/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8192 - accuracy: 0.6312 - val_loss: 0.7922 - val_accuracy: 0.6495\nEpoch 232/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8173 - accuracy: 0.6353 - val_loss: 0.7941 - val_accuracy: 0.6413\nEpoch 233/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8172 - accuracy: 0.6336 - val_loss: 0.7870 - val_accuracy: 0.6473\nEpoch 234/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8145 - accuracy: 0.6342 - val_loss: 0.7861 - val_accuracy: 0.6439\nEpoch 235/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8157 - accuracy: 0.6327 - val_loss: 0.7878 - val_accuracy: 0.6514\nEpoch 236/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8154 - accuracy: 0.6380 - val_loss: 0.8359 - val_accuracy: 0.6163\nEpoch 237/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8155 - accuracy: 0.6372 - val_loss: 0.7842 - val_accuracy: 0.6543\nEpoch 238/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8131 - accuracy: 0.6351 - val_loss: 0.7811 - val_accuracy: 0.6555\nEpoch 239/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8145 - accuracy: 0.6351 - val_loss: 0.7977 - val_accuracy: 0.6454\nEpoch 240/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8104 - accuracy: 0.6401 - val_loss: 0.8014 - val_accuracy: 0.6454\nEpoch 241/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8098 - accuracy: 0.6370 - val_loss: 0.7994 - val_accuracy: 0.6390\nEpoch 242/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8095 - accuracy: 0.6421 - val_loss: 0.7845 - val_accuracy: 0.6529\nEpoch 243/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8080 - accuracy: 0.6408 - val_loss: 0.7845 - val_accuracy: 0.6521\nEpoch 244/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8067 - accuracy: 0.6421 - val_loss: 0.7815 - val_accuracy: 0.6514\nEpoch 245/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8105 - accuracy: 0.6400 - val_loss: 0.7891 - val_accuracy: 0.6562\nEpoch 246/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8076 - accuracy: 0.6431 - val_loss: 0.7748 - val_accuracy: 0.6566\nEpoch 247/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8030 - accuracy: 0.6416 - val_loss: 0.8211 - val_accuracy: 0.6323\nEpoch 248/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8078 - accuracy: 0.6480 - val_loss: 0.7739 - val_accuracy: 0.6521\nEpoch 249/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8033 - accuracy: 0.6413 - val_loss: 0.7730 - val_accuracy: 0.6558\nEpoch 250/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8045 - accuracy: 0.6437 - val_loss: 0.7720 - val_accuracy: 0.6607\nEpoch 251/1000\n804/804 [==============================] - 2s 3ms/step - loss: 0.8017 - accuracy: 0.6418 - val_loss: 0.7916 - val_accuracy: 0.6469\nEpoch 252/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8054 - accuracy: 0.6434 - val_loss: 0.7739 - val_accuracy: 0.6543\nEpoch 253/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8055 - accuracy: 0.6463 - val_loss: 0.7778 - val_accuracy: 0.6540\nEpoch 254/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8031 - accuracy: 0.6451 - val_loss: 0.7835 - val_accuracy: 0.6543\nEpoch 255/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8001 - accuracy: 0.6465 - val_loss: 0.7979 - val_accuracy: 0.6473\nEpoch 256/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7990 - accuracy: 0.6469 - val_loss: 0.7851 - val_accuracy: 0.6499\nEpoch 257/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8012 - accuracy: 0.6460 - val_loss: 0.7849 - val_accuracy: 0.6506\nEpoch 258/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8006 - accuracy: 0.6484 - val_loss: 0.7879 - val_accuracy: 0.6473\nEpoch 259/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.8013 - accuracy: 0.6473 - val_loss: 0.7725 - val_accuracy: 0.6555\nEpoch 260/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7987 - accuracy: 0.6449 - val_loss: 0.7707 - val_accuracy: 0.6670\nEpoch 261/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7996 - accuracy: 0.6412 - val_loss: 0.7693 - val_accuracy: 0.6633\nEpoch 262/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7996 - accuracy: 0.6431 - val_loss: 0.7899 - val_accuracy: 0.6510\nEpoch 263/1000\n804/804 [==============================] - 2s 3ms/step - loss: 0.7985 - accuracy: 0.6477 - val_loss: 0.7732 - val_accuracy: 0.6592\nEpoch 264/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7993 - accuracy: 0.6452 - val_loss: 0.7735 - val_accuracy: 0.6573\nEpoch 265/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7988 - accuracy: 0.6490 - val_loss: 0.7941 - val_accuracy: 0.6506\nEpoch 266/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7957 - accuracy: 0.6524 - val_loss: 0.7747 - val_accuracy: 0.6588\nEpoch 267/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7955 - accuracy: 0.6518 - val_loss: 0.7796 - val_accuracy: 0.6502\nEpoch 268/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7945 - accuracy: 0.6480 - val_loss: 0.7690 - val_accuracy: 0.6585\nEpoch 269/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7990 - accuracy: 0.6467 - val_loss: 0.7671 - val_accuracy: 0.6641\nEpoch 270/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7967 - accuracy: 0.6454 - val_loss: 0.7791 - val_accuracy: 0.6562\nEpoch 271/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7951 - accuracy: 0.6508 - val_loss: 0.7682 - val_accuracy: 0.6588\nEpoch 272/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7946 - accuracy: 0.6534 - val_loss: 0.7808 - val_accuracy: 0.6592\nEpoch 273/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7962 - accuracy: 0.6472 - val_loss: 0.7774 - val_accuracy: 0.6637\nEpoch 274/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7914 - accuracy: 0.6556 - val_loss: 0.7880 - val_accuracy: 0.6469\nEpoch 275/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7963 - accuracy: 0.6503 - val_loss: 0.7681 - val_accuracy: 0.6644\nEpoch 276/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7962 - accuracy: 0.6484 - val_loss: 0.7730 - val_accuracy: 0.6603\nEpoch 277/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7926 - accuracy: 0.6502 - val_loss: 0.7675 - val_accuracy: 0.6626\nEpoch 278/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7947 - accuracy: 0.6493 - val_loss: 0.7849 - val_accuracy: 0.6502\nEpoch 279/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7951 - accuracy: 0.6489 - val_loss: 0.7878 - val_accuracy: 0.6566\nEpoch 280/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7937 - accuracy: 0.6513 - val_loss: 0.7814 - val_accuracy: 0.6473\nEpoch 281/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7916 - accuracy: 0.6507 - val_loss: 0.7680 - val_accuracy: 0.6607\nEpoch 282/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7905 - accuracy: 0.6541 - val_loss: 0.7583 - val_accuracy: 0.6682\nEpoch 283/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7877 - accuracy: 0.6545 - val_loss: 0.7683 - val_accuracy: 0.6622\nEpoch 284/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7901 - accuracy: 0.6536 - val_loss: 0.7745 - val_accuracy: 0.6697\nEpoch 285/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7900 - accuracy: 0.6523 - val_loss: 0.7641 - val_accuracy: 0.6659\nEpoch 286/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7892 - accuracy: 0.6535 - val_loss: 0.7644 - val_accuracy: 0.6629\nEpoch 287/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7909 - accuracy: 0.6518 - val_loss: 0.7587 - val_accuracy: 0.6670\nEpoch 288/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7872 - accuracy: 0.6556 - val_loss: 0.7772 - val_accuracy: 0.6529\nEpoch 289/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7906 - accuracy: 0.6526 - val_loss: 0.7688 - val_accuracy: 0.6644\nEpoch 290/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7900 - accuracy: 0.6494 - val_loss: 0.7615 - val_accuracy: 0.6629\nEpoch 291/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7873 - accuracy: 0.6528 - val_loss: 0.7937 - val_accuracy: 0.6540\nEpoch 292/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7884 - accuracy: 0.6544 - val_loss: 0.7601 - val_accuracy: 0.6689\nEpoch 293/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7912 - accuracy: 0.6492 - val_loss: 0.7652 - val_accuracy: 0.6670\nEpoch 294/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7886 - accuracy: 0.6498 - val_loss: 0.7648 - val_accuracy: 0.6764\nEpoch 295/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7880 - accuracy: 0.6549 - val_loss: 0.7898 - val_accuracy: 0.6435\nEpoch 296/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7849 - accuracy: 0.6586 - val_loss: 0.7666 - val_accuracy: 0.6603\nEpoch 297/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7854 - accuracy: 0.6574 - val_loss: 0.7687 - val_accuracy: 0.6655\nEpoch 298/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7836 - accuracy: 0.6595 - val_loss: 0.7694 - val_accuracy: 0.6734\nEpoch 299/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7857 - accuracy: 0.6571 - val_loss: 0.7851 - val_accuracy: 0.6532\nEpoch 300/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7866 - accuracy: 0.6515 - val_loss: 0.7732 - val_accuracy: 0.6715\nEpoch 301/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7869 - accuracy: 0.6529 - val_loss: 0.7953 - val_accuracy: 0.6413\nEpoch 302/1000\n804/804 [==============================] - 2s 2ms/step - loss: 0.7855 - accuracy: 0.6561 - val_loss: 0.7652 - val_accuracy: 0.6599\n84/84 [==============================] - 0s 1ms/step - loss: 0.7669 - accuracy: 0.6682\nTest accuracy: 0.6681597828865051\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}